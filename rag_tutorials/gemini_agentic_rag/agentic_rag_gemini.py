import os
import tempfile
from datetime import datetime
from typing import List

import streamlit as st
import google.generativeai as genai
import bs4
from agno.agent import Agent
from agno.models.google import Gemini
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from langchain_core.embeddings import Embeddings
from agno.tools.exa import ExaTools


class GeminiEmbedder(Embeddings):
    def __init__(self, model_name="models/text-embedding-004"):
        genai.configure(api_key=st.session_state.google_api_key)
        self.model = model_name

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]

    def embed_query(self, text: str) -> List[float]:
        response = genai.embed_content(
            model=self.model,
            content=text,
            task_type="retrieval_document"
        )
        return response['embedding']


# å¸¸é‡
COLLECTION_NAME = "gemini-thinking-agent-agno"

# Streamlitåº”ç”¨åˆå§‹åŒ–
st.title("ğŸ¤” åŸºäºGeminiå’ŒAgnoçš„æ™ºèƒ½RAGä»£ç†")

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if 'google_api_key' not in st.session_state:
    st.session_state.google_api_key = ""
if 'qdrant_api_key' not in st.session_state:
    st.session_state.qdrant_api_key = ""
if 'qdrant_url' not in st.session_state:
    st.session_state.qdrant_url = ""
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'processed_documents' not in st.session_state:
    st.session_state.processed_documents = []
if 'history' not in st.session_state:
    st.session_state.history = []
if 'exa_api_key' not in st.session_state:
    st.session_state.exa_api_key = ""
if 'use_web_search' not in st.session_state:
    st.session_state.use_web_search = False
if 'force_web_search' not in st.session_state:
    st.session_state.force_web_search = False
if 'similarity_threshold' not in st.session_state:
    st.session_state.similarity_threshold = 0.7

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("ğŸ”‘ APIé…ç½®")
google_api_key = st.sidebar.text_input("Google APIå¯†é’¥", type="password", value=st.session_state.google_api_key)
qdrant_api_key = st.sidebar.text_input("Qdrant APIå¯†é’¥", type="password", value=st.session_state.qdrant_api_key)
qdrant_url = st.sidebar.text_input("Qdrant URL",
                                   placeholder="https://your-cluster.cloud.qdrant.io:6333",
                                   value=st.session_state.qdrant_url)

# æ¸…é™¤èŠå¤©æŒ‰é’®
if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤èŠå¤©å†å²"):
    st.session_state.history = []
    st.rerun()

# æ›´æ–°ä¼šè¯çŠ¶æ€
st.session_state.google_api_key = google_api_key
st.session_state.qdrant_api_key = qdrant_api_key
st.session_state.qdrant_url = qdrant_url

# åœ¨ä¾§è¾¹æ é…ç½®éƒ¨åˆ†æ·»åŠ ï¼Œåœ¨ç°æœ‰APIè¾“å…¥ä¹‹å
st.sidebar.header("ğŸŒ ç½‘ç»œæœç´¢é…ç½®")
st.session_state.use_web_search = st.sidebar.checkbox("å¯ç”¨ç½‘ç»œæœç´¢fallback", value=st.session_state.use_web_search)

if st.session_state.use_web_search:
    exa_api_key = st.sidebar.text_input(
        "Exa AI APIå¯†é’¥",
        type="password",
        value=st.session_state.exa_api_key,
        help="å½“æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ—¶ï¼Œç½‘ç»œæœç´¢fallbackéœ€è¦æ­¤å¯†é’¥"
    )
    st.session_state.exa_api_key = exa_api_key

    # å¯é€‰çš„åŸŸåè¿‡æ»¤
    default_domains = ["arxiv.org", "wikipedia.org", "github.com", "medium.com"]
    custom_domains = st.sidebar.text_input(
        "è‡ªå®šä¹‰åŸŸåï¼ˆé€—å·åˆ†éš”ï¼‰",
        value=",".join(default_domains),
        help="è¾“å…¥è¦æœç´¢çš„åŸŸåï¼Œä¾‹å¦‚ï¼šarxiv.org,wikipedia.org"
    )
    search_domains = [d.strip() for d in custom_domains.split(",") if d.strip()]

# æ·»åŠ åˆ°ä¾§è¾¹æ é…ç½®éƒ¨åˆ†
st.sidebar.header("ğŸ¯ æœç´¢é…ç½®")
st.session_state.similarity_threshold = st.sidebar.slider(
    "æ–‡æ¡£ç›¸ä¼¼åº¦é˜ˆå€¼",
    min_value=0.0,
    max_value=1.0,
    value=0.7,
    help="è¾ƒä½çš„å€¼å°†è¿”å›æ›´å¤šæ–‡æ¡£ä½†ç›¸å…³æ€§å¯èƒ½è¾ƒä½ã€‚è¾ƒé«˜çš„å€¼æ›´ä¸¥æ ¼ã€‚"
)


# å·¥å…·å‡½æ•°
def init_qdrant():
    """ä½¿ç”¨é…ç½®çš„è®¾ç½®åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯ã€‚"""
    if not all([st.session_state.qdrant_api_key, st.session_state.qdrant_url]):
        return None
    try:
        return QdrantClient(
            url=st.session_state.qdrant_url,
            api_key=st.session_state.qdrant_api_key,
            timeout=60
        )
    except Exception as e:
        st.error(f"ğŸ”´ Qdrantè¿æ¥å¤±è´¥: {str(e)}")
        return None


# æ–‡æ¡£å¤„ç†å‡½æ•°
def process_pdf(file) -> List:
    """å¤„ç†PDFæ–‡ä»¶å¹¶æ·»åŠ æºå…ƒæ•°æ®ã€‚"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(file.getvalue())
            loader = PyPDFLoader(tmp_file.name)
            documents = loader.load()

            # æ·»åŠ æºå…ƒæ•°æ®
            for doc in documents:
                doc.metadata.update({
                    "source_type": "pdf",
                    "file_name": file.name,
                    "timestamp": datetime.now().isoformat()
                })

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            return text_splitter.split_documents(documents)
    except Exception as e:
        st.error(f"ğŸ“„ PDFå¤„ç†é”™è¯¯: {str(e)}")
        return []


def process_web(url: str) -> List:
    """å¤„ç†ç½‘é¡µURLå¹¶æ·»åŠ æºå…ƒæ•°æ®ã€‚"""
    try:
        loader = WebBaseLoader(
            web_paths=(url,),
            bs_kwargs=dict(
                parse_only=bs4.SoupStrainer(
                    class_=("post-content", "post-title", "post-header", "content", "main")
                )
            )
        )
        documents = loader.load()

        # æ·»åŠ æºå…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update({
                "source_type": "url",
                "url": url,
                "timestamp": datetime.now().isoformat()
            })

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        return text_splitter.split_documents(documents)
    except Exception as e:
        st.error(f"ğŸŒ ç½‘é¡µå¤„ç†é”™è¯¯: {str(e)}")
        return []


# å‘é‡å­˜å‚¨ç®¡ç†
def create_vector_store(client, texts):
    """åˆ›å»ºå¹¶åˆå§‹åŒ–å¸¦æœ‰æ–‡æ¡£çš„å‘é‡å­˜å‚¨ã€‚"""
    try:
        # å¿…è¦æ—¶åˆ›å»ºé›†åˆ
        try:
            client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(
                    size=768,  # Gemini embedding-004ç»´åº¦
                    distance=Distance.COSINE
                )
            )
            st.success(f"ğŸ“š åˆ›å»ºæ–°é›†åˆ: {COLLECTION_NAME}")
        except Exception as e:
            if "already exists" not in str(e).lower():
                raise e

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store = QdrantVectorStore(
            client=client,
            collection_name=COLLECTION_NAME,
            embedding=GeminiEmbedder()
        )

        # æ·»åŠ æ–‡æ¡£
        with st.spinner('ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡æ¡£åˆ°Qdrant...'):
            vector_store.add_documents(texts)
            st.success("âœ… æ–‡æ¡£å­˜å‚¨æˆåŠŸï¼")
            return vector_store

    except Exception as e:
        st.error(f"ğŸ”´ å‘é‡å­˜å‚¨é”™è¯¯: {str(e)}")
        return None


# åœ¨GeminiEmbedderç±»ä¹‹åæ·»åŠ 
def get_query_rewriter_agent() -> Agent:
    """åˆå§‹åŒ–æŸ¥è¯¢é‡å†™ä»£ç†ã€‚"""
    return Agent(
        name="æŸ¥è¯¢é‡å†™å™¨",
        model=Gemini(id="gemini-exp-1206"),
        instructions="""ä½ æ˜¯å°†é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ›´ç²¾ç¡®å’Œè¯¦ç»†çš„ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯ï¼š
        1. åˆ†æç”¨æˆ·çš„é—®é¢˜
        2. å°†å…¶é‡å†™ä¸ºæ›´å…·ä½“å’Œæœç´¢å‹å¥½çš„å½¢å¼
        3. æ‰©å±•ä»»ä½•ç¼©å†™è¯æˆ–æŠ€æœ¯æœ¯è¯­
        4. åªè¿”å›é‡å†™åçš„æŸ¥è¯¢ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š

        ç¤ºä¾‹1:
        ç”¨æˆ·: "What does it say about ML?"
        è¾“å‡º: "What are the key concepts, techniques, and applications of Machine Learning (ML) discussed in the context?"

        ç¤ºä¾‹2:
        ç”¨æˆ·: "Tell me about transformers"
        è¾“å‡º: "Explain the architecture, mechanisms, and applications of Transformer neural networks in natural language processing and deep learning"
        """,
        show_tool_calls=False,
        markdown=True,
    )


def get_web_search_agent() -> Agent:
    """åˆå§‹åŒ–ç½‘ç»œæœç´¢ä»£ç†ã€‚"""
    return Agent(
        name="ç½‘ç»œæœç´¢ä»£ç†",
        model=Gemini(id="gemini-exp-1206"),
        tools=[ExaTools(
            api_key=st.session_state.exa_api_key,
            include_domains=search_domains,
            num_results=5
        )],
        instructions="""ä½ æ˜¯ç½‘ç»œæœç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
        1. åœ¨ç½‘ç»œä¸Šæœç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯
        2. æ±‡ç¼–å¹¶æ€»ç»“æœ€ç›¸å…³çš„ä¿¡æ¯
        3. åœ¨ä½ çš„å›ç­”ä¸­åŒ…å«æ¥æº
        """,
        show_tool_calls=True,
        markdown=True,
    )


def get_rag_agent() -> Agent:
    """åˆå§‹åŒ–ä¸»RAGä»£ç†ã€‚"""
    return Agent(
        name="Gemini RAGä»£ç†",
        model=Gemini(id="gemini-2.0-flash-thinking-exp-01-21"),
        instructions="""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç†ï¼Œä¸“é—¨æä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚

        å½“ç»™å‡ºæ–‡æ¡£ä¸Šä¸‹æ–‡æ—¶ï¼š
        - ä¸“æ³¨äºæä¾›çš„æ–‡æ¡£ä¸­çš„ä¿¡æ¯
        - è¦ç²¾ç¡®å¹¶å¼•ç”¨å…·ä½“ç»†èŠ‚

        å½“ç»™å‡ºç½‘ç»œæœç´¢ç»“æœæ—¶ï¼š
        - æ¸…æ¥šåœ°è¡¨æ˜ä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢
        - æ¸…æ™°åœ°ç»¼åˆä¿¡æ¯

        å§‹ç»ˆä¿æŒå›ç­”çš„é«˜åº¦å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ã€‚
        """,
        show_tool_calls=True,
        markdown=True,
    )


def check_document_relevance(query: str, vector_store, threshold: float = 0.7) -> tuple[bool, List]:
    """
    æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³ã€‚

    å‚æ•°:
        query: æœç´¢æŸ¥è¯¢
        vector_store: è¦æœç´¢çš„å‘é‡å­˜å‚¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    è¿”å›:
        tuple[bool, List]: (æ˜¯å¦æœ‰å…³è”æ–‡æ¡£, å…³è”æ–‡æ¡£åˆ—è¡¨)
    """
    if not vector_store:
        return False, []

    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 5, "score_threshold": threshold}
    )
    docs = retriever.invoke(query)
    return bool(docs), docs


# ä¸»åº”ç”¨æµç¨‹
if st.session_state.google_api_key:
    os.environ["GOOGLE_API_KEY"] = st.session_state.google_api_key
    genai.configure(api_key=st.session_state.google_api_key)

    qdrant_client = init_qdrant()

    # æ–‡ä»¶/URLä¸Šä¼ éƒ¨åˆ†
    st.sidebar.header("ğŸ“ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ PDF", type=["pdf"])
    web_url = st.sidebar.text_input("æˆ–è¾“å…¥URL")

    # å¤„ç†æ–‡æ¡£
    if uploaded_file:
        file_name = uploaded_file.name
        if file_name not in st.session_state.processed_documents:
            with st.spinner('å¤„ç†PDFä¸­...'):
                texts = process_pdf(uploaded_file)
                if texts and qdrant_client:
                    if st.session_state.vector_store:
                        st.session_state.vector_store.add_documents(texts)
                    else:
                        st.session_state.vector_store = create_vector_store(qdrant_client, texts)
                    st.session_state.processed_documents.append(file_name)
                    st.success(f"âœ… æ·»åŠ PDF: {file_name}")

    if web_url:
        if web_url not in st.session_state.processed_documents:
            with st.spinner('å¤„ç†URLä¸­...'):
                texts = process_web(web_url)
                if texts and qdrant_client:
                    if st.session_state.vector_store:
                        st.session_state.vector_store.add_documents(texts)
                    else:
                        st.session_state.vector_store = create_vector_store(qdrant_client, texts)
                    st.session_state.processed_documents.append(web_url)
                    st.success(f"âœ… æ·»åŠ URL: {web_url}")

    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºæ¥æº
    if st.session_state.processed_documents:
        st.sidebar.header("ğŸ“š å·²å¤„ç†æ¥æº")
        for source in st.session_state.processed_documents:
            if source.endswith('.pdf'):
                st.sidebar.text(f"ğŸ“„ {source}")
            else:
                st.sidebar.text(f"ğŸŒ {source}")

    # èŠå¤©ç•Œé¢
    # åˆ›å»ºä¸¤åˆ—ç”¨äºèŠå¤©è¾“å…¥å’Œæœç´¢åˆ‡æ¢
    chat_col, toggle_col = st.columns([0.9, 0.1])

    with chat_col:
        prompt = st.chat_input("è¯¢é—®å…³äºä½ çš„æ–‡æ¡£...")

    with toggle_col:
        st.session_state.force_web_search = st.toggle('ğŸŒ', help="å¼ºåˆ¶ç½‘ç»œæœç´¢")

    if prompt:
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # æ­¥éª¤1: é‡å†™æŸ¥è¯¢ä»¥è·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœ
        with st.spinner("ğŸ¤” é‡æ–°è¡¨è¿°æŸ¥è¯¢..."):
            try:
                query_rewriter = get_query_rewriter_agent()
                rewritten_query = query_rewriter.run(prompt).content

                with st.expander("ğŸ”„ æŸ¥çœ‹é‡å†™åçš„æŸ¥è¯¢"):
                    st.write(f"åŸå§‹: {prompt}")
                    st.write(f"é‡å†™: {rewritten_query}")
            except Exception as e:
                st.error(f"âŒ é‡å†™æŸ¥è¯¢é”™è¯¯: {str(e)}")
                rewritten_query = prompt

        # æ­¥éª¤2: åŸºäºforce_web_searchåˆ‡æ¢é€‰æ‹©æœç´¢ç­–ç•¥
        context = ""
        docs = []
        if not st.session_state.force_web_search and st.session_state.vector_store:
            # é¦–å…ˆå°è¯•æ–‡æ¡£æœç´¢
            retriever = st.session_state.vector_store.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "k": 5,
                    "score_threshold": st.session_state.similarity_threshold
                }
            )
            docs = retriever.invoke(rewritten_query)
            if docs:
                context = "\n\n".join([d.page_content for d in docs])
                st.info(f"ğŸ“Š æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ (ç›¸ä¼¼åº¦ > {st.session_state.similarity_threshold})")
            elif st.session_state.use_web_search:
                st.info("ğŸ”„ åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œfallbackåˆ°ç½‘ç»œæœç´¢...")

        # æ­¥éª¤3: åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨ç½‘ç»œæœç´¢:
        # 1. é€šè¿‡åˆ‡æ¢å¼ºåˆ¶å¯ç”¨ç½‘ç»œæœç´¢ï¼Œæˆ–è€…
        # 2. æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¸”åœ¨è®¾ç½®ä¸­å¯ç”¨äº†ç½‘ç»œæœç´¢
        if (
                st.session_state.force_web_search or not context) and st.session_state.use_web_search and st.session_state.exa_api_key:
            with st.spinner("ğŸ” æ­£åœ¨æœç´¢ç½‘ç»œ..."):
                try:
                    web_search_agent = get_web_search_agent()
                    web_results = web_search_agent.run(rewritten_query).content
                    if web_results:
                        context = f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_results}"
                        if st.session_state.force_web_search:
                            st.info("â„¹ï¸ æŒ‰åˆ‡æ¢è¯·æ±‚ä½¿ç”¨ç½‘ç»œæœç´¢ã€‚")
                        else:
                            st.info("â„¹ï¸ ç”±äºæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨ç½‘ç»œæœç´¢ä½œä¸ºfallbackã€‚")
                except Exception as e:
                    st.error(f"âŒ ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")

        # æ­¥éª¤4: ä½¿ç”¨RAGä»£ç†ç”Ÿæˆå“åº”
        with st.spinner("ğŸ¤– æ€è€ƒä¸­..."):
            try:
                rag_agent = get_rag_agent()

                if context:
                    full_prompt = f"""ä¸Šä¸‹æ–‡: {context}

åŸå§‹é—®é¢˜: {prompt}
é‡å†™é—®é¢˜: {rewritten_query}

è¯·åŸºäºå¯ç”¨ä¿¡æ¯æä¾›å…¨é¢çš„ç­”æ¡ˆã€‚"""
                else:
                    full_prompt = f"åŸå§‹é—®é¢˜: {prompt}\né‡å†™é—®é¢˜: {rewritten_query}"
                    st.info("â„¹ï¸ åœ¨æ–‡æ¡£æˆ–ç½‘ç»œæœç´¢ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

                response = rag_agent.run(full_prompt)

                # å°†åŠ©æ‰‹å“åº”æ·»åŠ åˆ°å†å²è®°å½•
                st.session_state.history.append({
                    "role": "assistant",
                    "content": response.content
                })

                # æ˜¾ç¤ºåŠ©æ‰‹å“åº”
                with st.chat_message("assistant"):
                    st.write(response.content)

                    # å¦‚æœæœ‰æ¥æºåˆ™æ˜¾ç¤º
                    if not st.session_state.force_web_search and 'docs' in locals() and docs:
                        with st.expander("ğŸ” æŸ¥çœ‹æ–‡æ¡£æ¥æº"):
                            for i, doc in enumerate(docs, 1):
                                source_type = doc.metadata.get("source_type", "unknown")
                                source_icon = "ğŸ“„" if source_type == "pdf" else "ğŸŒ"
                                source_name = doc.metadata.get("file_name" if source_type == "pdf" else "url",
                                                               "unknown")
                                st.write(f"{source_icon} æ¥æº {i} æ¥è‡ª {source_name}:")
                                st.write(f"{doc.page_content[:200]}...")

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå“åº”é”™è¯¯: {str(e)}")

else:
    st.warning("âš ï¸ è¯·è¾“å…¥æ‚¨çš„Google APIå¯†é’¥ä»¥ç»§ç»­")