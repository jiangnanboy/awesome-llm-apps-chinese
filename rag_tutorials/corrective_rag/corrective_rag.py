from langchain import hub
from langchain.output_parsers import PydanticOutputParser
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document
from pydantic import BaseModel, Field
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader
from langchain_community.tools import TavilySearchResults
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import END, StateGraph
from typing import Dict, TypedDict
from langchain_core.prompts import PromptTemplate
import pprint
import yaml
import nest_asyncio
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
import tempfile
import os
from langchain_anthropic import ChatAnthropic
from tenacity import retry, stop_after_attempt, wait_exponential

nest_asyncio.apply()

retriever = None


def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œç”¨äºå­˜å‚¨APIå¯†é’¥å’ŒURL"""
    if 'initialized' not in st.session_state:
        st.session_state.initialized = False
        # åˆå§‹åŒ–APIå¯†é’¥å’ŒURL
        st.session_state.anthropic_api_key = ""
        st.session_state.openai_api_key = ""
        st.session_state.tavily_api_key = ""
        st.session_state.qdrant_api_key = ""
        st.session_state.qdrant_url = "http://localhost:6333"
        st.session_state.doc_url = "https://arxiv.org/pdf/2307.09288.pdf"


def setup_sidebar():
    """è®¾ç½®ä¾§è¾¹æ ï¼Œç”¨äºè¾“å…¥APIå¯†é’¥å’Œé…ç½®ä¿¡æ¯"""
    with st.sidebar:
        st.subheader("APIé…ç½®")
        st.session_state.anthropic_api_key = st.text_input("Anthropic APIå¯†é’¥", value=st.session_state.anthropic_api_key,
                                                           type="password", help="Claude 3æ¨¡å‹æ‰€éœ€")
        st.session_state.openai_api_key = st.text_input("OpenAI APIå¯†é’¥", value=st.session_state.openai_api_key,
                                                        type="password")
        st.session_state.tavily_api_key = st.text_input("Tavily APIå¯†é’¥", value=st.session_state.tavily_api_key,
                                                        type="password")
        st.session_state.qdrant_url = st.text_input("Qdrant URL", value=st.session_state.qdrant_url)
        st.session_state.qdrant_api_key = st.text_input("Qdrant APIå¯†é’¥", value=st.session_state.qdrant_api_key,
                                                        type="password")
        st.session_state.doc_url = st.text_input("æ–‡æ¡£URL", value=st.session_state.doc_url)

        if not all([st.session_state.openai_api_key, st.session_state.anthropic_api_key, st.session_state.qdrant_url]):
            st.warning("è¯·æä¾›æ‰€éœ€çš„APIå¯†é’¥å’ŒURL")
            st.stop()

        st.session_state.initialized = True


initialize_session_state()
setup_sidebar()

# ä½¿ç”¨ä¼šè¯çŠ¶æ€å˜é‡è€Œéé…ç½®
openai_api_key = st.session_state.openai_api_key
tavily_api_key = st.session_state.tavily_api_key
anthropic_api_key = st.session_state.anthropic_api_key

# æ›´æ–°åµŒå…¥åˆå§‹åŒ–
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=st.session_state.openai_api_key
)

# æ›´æ–°Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–
client = QdrantClient(
    url=st.session_state.qdrant_url,
    api_key=st.session_state.qdrant_api_key
)


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def execute_tavily_search(tool, query):
    return tool.invoke({"query": query})


def web_search(state):
    """åŸºäºé‡æ–°è¡¨è¿°çš„é—®é¢˜ä½¿ç”¨Tavily APIè¿›è¡Œç½‘ç»œæœç´¢"""
    print("~-ç½‘ç»œæœç´¢-~")
    state_dict = state["keys"]
    question = state_dict["question"]
    documents = state_dict["documents"]

    # åˆ›å»ºè¿›åº¦å ä½ç¬¦
    progress_placeholder = st.empty()
    progress_placeholder.info("æ­£åœ¨åˆå§‹åŒ–ç½‘ç»œæœç´¢...")

    try:
        # éªŒè¯Tavily APIå¯†é’¥
        if not st.session_state.tavily_api_key:
            progress_placeholder.warning("æœªæä¾›Tavily APIå¯†é’¥ - è·³è¿‡ç½‘ç»œæœç´¢")
            return {"keys": {"documents": documents, "question": question}}

        progress_placeholder.info("æ­£åœ¨é…ç½®æœç´¢å·¥å…·...")

        # åˆå§‹åŒ–Tavilyæœç´¢å·¥å…·
        tool = TavilySearchResults(
            api_key=st.session_state.tavily_api_key,
            max_results=3,
            search_depth="advanced"
        )

        # ä½¿ç”¨é‡è¯•é€»è¾‘æ‰§è¡Œæœç´¢
        progress_placeholder.info("æ­£åœ¨æ‰§è¡Œæœç´¢æŸ¥è¯¢...")
        try:
            search_results = execute_tavily_search(tool, question)
        except Exception as search_error:
            progress_placeholder.error(f"å¤šæ¬¡å°è¯•åæœç´¢å¤±è´¥: {str(search_error)}")
            return {"keys": {"documents": documents, "question": question}}

        if not search_results:
            progress_placeholder.warning("æœªæ‰¾åˆ°æœç´¢ç»“æœ")
            return {"keys": {"documents": documents, "question": question}}

        # å¤„ç†ç»“æœ
        progress_placeholder.info("æ­£åœ¨å¤„ç†æœç´¢ç»“æœ...")
        web_results = []
        for result in search_results:
            # æå–å¹¶æ ¼å¼åŒ–ç›¸å…³ä¿¡æ¯
            content = (
                f"æ ‡é¢˜: {result.get('title', 'æ— æ ‡é¢˜')}\n"
                f"å†…å®¹: {result.get('content', 'æ— å†…å®¹')}\n"
            )
            web_results.append(content)

        # ä»ç»“æœåˆ›å»ºæ–‡æ¡£
        web_document = Document(
            page_content="\n\n".join(web_results),
            metadata={
                "æ¥æº": "tavily_search",
                "æŸ¥è¯¢": question,
                "ç»“æœæ•°é‡": len(web_results)
            }
        )
        documents.append(web_document)

        progress_placeholder.success(f"æˆåŠŸæ·»åŠ  {len(web_results)} æ¡æœç´¢ç»“æœ")

    except Exception as error:
        error_msg = f"ç½‘ç»œæœç´¢é”™è¯¯: {str(error)}"
        print(error_msg)
        progress_placeholder.error(error_msg)
    finally:
        progress_placeholder.empty()
    return {"keys": {"documents": documents, "question": question}}


def load_documents(file_or_url: str, is_url: bool = True) -> list:
    try:
        if is_url:
            loader = WebBaseLoader(file_or_url)
            loader.requests_per_second = 1
        else:
            file_extension = os.path.splitext(file_or_url)[1].lower()
            if file_extension == '.pdf':
                loader = PyPDFLoader(file_or_url)
            elif file_extension in ['.txt', '.md']:
                loader = TextLoader(file_or_url)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}")

        return loader.load()
    except Exception as e:
        st.error(f"åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
        return []


st.subheader("æ–‡æ¡£è¾“å…¥")
input_option = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼:", ["URL", "æ–‡ä»¶ä¸Šä¼ "])

docs = None

if input_option == "URL":
    url = st.text_input("è¾“å…¥æ–‡æ¡£URL:", value=st.session_state.doc_url)
    if url:
        docs = load_documents(url, is_url=True)
else:
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=['pdf', 'txt', 'md'])
    if uploaded_file:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨ä¸Šä¼ å†…å®¹
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            docs = load_documents(tmp_file.name, is_url=False)
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_file.name)

if docs:
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=500, chunk_overlap=100
    )
    all_splits = text_splitter.split_documents(docs)

    client = QdrantClient(url=st.session_state.qdrant_url, api_key=st.session_state.qdrant_api_key)
    collection_name = "rag-qdrant"

    try:
        # å¦‚æœé›†åˆå­˜åœ¨ï¼Œå°è¯•åˆ é™¤å®ƒ
        client.delete_collection(collection_name)
    except Exception:
        pass

    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
    )

    # åˆ›å»ºå‘é‡å­˜å‚¨
    vectorstore = Qdrant(
        client=client,
        collection_name=collection_name,
        embeddings=embeddings,
    )

    # å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨
    vectorstore.add_documents(all_splits)
    retriever = vectorstore.as_retriever()


class GraphState(TypedDict):
    keys: Dict[str, any]


def retrieve(state):
    print("~-æ£€ç´¢-~")
    state_dict = state["keys"]
    question = state_dict["question"]

    if retriever is None:
        return {"keys": {"documents": [], "question": question}}

    documents = retriever.get_relevant_documents(question)
    return {"keys": {"documents": documents, "question": question}}


def generate(state):
    """ä½¿ç”¨Claude 3æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""
    print("~-ç”Ÿæˆ-~")
    state_dict = state["keys"]
    question, documents = state_dict["question"], state_dict["documents"]
    try:
        prompt = PromptTemplate(template="""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œè¯·å›ç­”é—®é¢˜ã€‚
            ä¸Šä¸‹æ–‡: {context}
            é—®é¢˜: {question}
            å›ç­”:""", input_variables=["context", "question"])
        llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", api_key=st.session_state.anthropic_api_key,
                            temperature=0, max_tokens=1000)
        context = "\n\n".join(doc.page_content for doc in documents)

        # åˆ›å»ºå¹¶è¿è¡Œé“¾
        rag_chain = (
                {"context": lambda x: context, "question": lambda x: question}
                | prompt
                | llm
                | StrOutputParser()
        )

        generation = rag_chain.invoke({})

        return {
            "keys": {
                "documents": documents,
                "question": question,
                "generation": generation
            }
        }

    except Exception as e:
        error_msg = f"ç”Ÿæˆå‡½æ•°å‡ºé”™: {str(e)}"
        print(error_msg)
        st.error(error_msg)
        return {"keys": {"documents": documents, "question": question,
                         "generation": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é”™è¯¯ã€‚"}}


def grade_documents(state):
    """åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³"""
    print("~-æ£€æŸ¥ç›¸å…³æ€§-~")
    state_dict = state["keys"]
    question = state_dict["question"]
    documents = state_dict["documents"]

    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", api_key=st.session_state.anthropic_api_key,
                        temperature=0, max_tokens=1000)

    prompt = PromptTemplate(template="""ä½ éœ€è¦è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚
        åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«"score"å­—æ®µï¼Œå€¼ä¸º"yes"æˆ–"no"ã€‚
        ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚

        æ–‡æ¡£: {context}
        é—®é¢˜: {question}

        è§„åˆ™:
        - æ£€æŸ¥ç›¸å…³å…³é”®è¯æˆ–è¯­ä¹‰å«ä¹‰
        - ä½¿ç”¨å®½æ¾çš„è¯„åˆ†æ ‡å‡†ï¼Œåªè¿‡æ»¤æ˜æ˜¾ä¸åŒ¹é…çš„å†…å®¹
        - ä¸¥æ ¼æŒ‰ç…§ç¤ºä¾‹æ ¼å¼è¿”å›: {{"score": "yes"}} æˆ– {{"score": "no"}}""",
                            input_variables=["context", "question"])

    chain = (
            prompt
            | llm
            | StrOutputParser()
    )

    filtered_docs = []
    search = "No"

    for d in documents:
        try:
            response = chain.invoke({"question": question, "context": d.page_content})
            import re
            json_match = re.search(r'\{.*\}', response)
            if json_match:
                response = json_match.group()

            import json
            score = json.loads(response)

            if score.get("score") == "yes":
                print("~-è¯„åˆ†: æ–‡æ¡£ç›¸å…³-~")
                filtered_docs.append(d)
            else:
                print("~-è¯„åˆ†: æ–‡æ¡£ä¸ç›¸å…³-~")
                search = "Yes"

        except Exception as e:
            print(f"è¯„ä¼°æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶ä¿ç•™æ–‡æ¡£ä»¥ç¡®ä¿å®‰å…¨
            filtered_docs.append(d)
            continue

    return {"keys": {"documents": filtered_docs, "question": question, "run_web_search": search}}


def transform_query(state):
    """è½¬æ¢æŸ¥è¯¢ä»¥ç”Ÿæˆæ›´å¥½çš„é—®é¢˜"""
    print("~-è½¬æ¢æŸ¥è¯¢-~")
    state_dict = state["keys"]
    question = state_dict["question"]
    documents = state_dict["documents"]

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = PromptTemplate(
        template="""é€šè¿‡åˆ†æé—®é¢˜çš„æ ¸å¿ƒè¯­ä¹‰å’Œæ„å›¾ï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–çš„æœç´¢ç‰ˆæœ¬ã€‚
        \n ------- \n
        {question}
        \n ------- \n
        åªè¿”å›æ”¹è¿›åçš„é—®é¢˜ï¼Œä¸è¦é™„åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬:""",
        input_variables=["question"],
    )

    # ä½¿ç”¨Claudeè€ŒéGemini
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20240620",
        anthropic_api_key=st.session_state.anthropic_api_key,
        temperature=0,
        max_tokens=1000
    )

    # æç¤º
    chain = prompt | llm | StrOutputParser()
    better_question = chain.invoke({"question": question})

    return {
        "keys": {"documents": documents, "question": better_question}
    }


def decide_to_generate(state):
    print("~-å†³å®šç”Ÿæˆ-~")
    state_dict = state["keys"]
    search = state_dict["run_web_search"]

    if search == "Yes":

        print("~-å†³å®š: è½¬æ¢æŸ¥è¯¢å¹¶è¿è¡Œç½‘ç»œæœç´¢-~")
        return "transform_query"
    else:
        print("~-å†³å®š: ç”Ÿæˆ-~")
        return "generate"


def format_document(doc: Document) -> str:
    return f"""
    æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}
    æ ‡é¢˜: {doc.metadata.get('title', 'æ— æ ‡é¢˜')}
    å†…å®¹: {doc.page_content[:200]}...
    """


def format_state(state: dict) -> str:
    formatted = {}

    for key, value in state.items():
        if key == "documents":
            formatted[key] = [format_document(doc) for doc in value]
        else:
            formatted[key] = value

    return formatted


workflow = StateGraph(GraphState)

# é€šè¿‡langgraphå®šä¹‰èŠ‚ç‚¹
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("transform_query", transform_query)
workflow.add_node("web_search", web_search)

# æ„å»ºå›¾
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    },
)
workflow.add_edge("transform_query", "web_search")
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()

st.title("ğŸ”„ æ ¡æ­£å‹RAGä»£ç†")

st.text("ä¸€ä¸ªç¤ºä¾‹æŸ¥è¯¢: è¿™ç¯‡ç ”ç©¶è®ºæ–‡ä¸­çš„å®éªŒç»“æœå’Œæ¶ˆèç ”ç©¶æ˜¯ä»€ä¹ˆ?")

# ç”¨æˆ·è¾“å…¥
user_question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜:")

if user_question:
    inputs = {
        "keys": {
            "question": user_question,
        }
    }

    for output in app.stream(inputs):
        for key, value in output.items():
            with st.expander(f"æ­¥éª¤ '{key}':"):
                st.text(pprint.pformat(format_state(value["keys"]), indent=2, width=80))

    final_generation = value['keys'].get('generation', 'æœªç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚')
    st.subheader("æœ€ç»ˆå›ç­”:")
    st.write(final_generation)