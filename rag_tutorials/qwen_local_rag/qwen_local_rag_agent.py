import os
import tempfile
from datetime import datetime
from typing import List
import streamlit as st
import bs4
from agno.agent import Agent
from agno.models.ollama import Ollama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from langchain_core.embeddings import Embeddings
from agno.tools.exa import ExaTools
from agno.embedder.ollama import OllamaEmbedder


class OllamaEmbedderr(Embeddings):
    def __init__(self, model_name="snowflake-arctic-embed"):
        """
        ä½¿ç”¨ç‰¹å®šæ¨¡å‹åˆå§‹åŒ–OllamaEmbedderrã€‚

        å‚æ•°:
            model_name (str): ç”¨äºç”ŸæˆåµŒå…¥å‘é‡çš„æ¨¡å‹åç§°ã€‚
        """
        self.embedder = OllamaEmbedder(id=model_name, dimensions=1024)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]

    def embed_query(self, text: str) -> List[float]:
        return self.embedder.get_embedding(text)


# å¸¸é‡å®šä¹‰
COLLECTION_NAME = "test-qwen-r1"

# Streamlitåº”ç”¨åˆå§‹åŒ–
st.title("ğŸ‹ Qwen 3 æœ¬åœ°RAGæ¨ç†åŠ©æ‰‹")

# --- æ·»åŠ æ¨¡å‹ä¿¡æ¯æ¡† ---
st.info("**Qwen3**ï¼šé€šä¹‰åƒé—®ç³»åˆ—æœ€æ–°ä¸€ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›å®Œæ•´çš„ç¨ å¯†æ¨¡å‹å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹å¥—ä»¶ã€‚")
st.info("**Gemma 3**ï¼šå¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ”¯æŒæ–‡æœ¬å’Œå›¾åƒå¤„ç†ï¼‰ï¼Œæ‹¥æœ‰128Kä¸Šä¸‹æ–‡çª—å£ï¼Œæ”¯æŒè¶…è¿‡140ç§è¯­è¨€ã€‚")
# -------------------------

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if 'model_version' not in st.session_state:
    st.session_state.model_version = "qwen3:1.7b"  # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å‹
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'processed_documents' not in st.session_state:
    st.session_state.processed_documents = []
if 'history' not in st.session_state:
    st.session_state.history = []
if 'exa_api_key' not in st.session_state:
    st.session_state.exa_api_key = ""
if 'use_web_search' not in st.session_state:
    st.session_state.use_web_search = False
if 'force_web_search' not in st.session_state:
    st.session_state.force_web_search = False
if 'similarity_threshold' not in st.session_state:
    st.session_state.similarity_threshold = 0.7
if 'rag_enabled' not in st.session_state:
    st.session_state.rag_enabled = True  # é»˜è®¤å¯ç”¨RAGåŠŸèƒ½

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("âš™ï¸ è®¾ç½®")

# æ¨¡å‹é€‰æ‹©
st.sidebar.header("ğŸ§  æ¨¡å‹é€‰æ‹©")
model_help = """
- qwen3:1.7b: è½»é‡çº§æ¨¡å‹ï¼ˆMoEæ¶æ„ï¼‰
- gemma3:1b: æ€§èƒ½æ›´å¼ºï¼Œä½†éœ€è¦æ›´å¥½çš„GPU/å†…å­˜ï¼ˆ32Kä¸Šä¸‹æ–‡çª—å£ï¼‰
- gemma3:4b: æ€§èƒ½æ›´å¼ºä¸”æ”¯æŒå¤šæ¨¡æ€ï¼ˆè§†è§‰ï¼‰ï¼ˆ128Kä¸Šä¸‹æ–‡çª—å£ï¼‰
- deepseek-r1:1.5b: æ·±åº¦æ±‚ç´¢R1æ¨¡å‹ï¼ˆ1.5Bå‚æ•°ï¼‰
- qwen3:8b: æ€§èƒ½æœ€å¼ºï¼Œä½†éœ€è¦é«˜æ€§èƒ½GPU/å†…å­˜

æ ¹æ®æ‚¨çš„ç¡¬ä»¶æ€§èƒ½é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚
"""
st.session_state.model_version = st.sidebar.radio(
    "é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬",
    options=["qwen3:1.7b", "gemma3:1b", "gemma3:4b", "deepseek-r1:1.5b", "qwen3:8b"],
    help=model_help
)

st.sidebar.info("è¯·å…ˆæ‰§è¡Œå‘½ä»¤æ‹‰å–æ¨¡å‹ï¼šollama pull qwen3:1.7b")

# RAGæ¨¡å¼åˆ‡æ¢
st.sidebar.header("ğŸ“š RAGæ¨¡å¼")
st.session_state.rag_enabled = st.sidebar.toggle("å¯ç”¨RAG", value=st.session_state.rag_enabled)

# æ¸…ç©ºèŠå¤©æŒ‰é’®
if st.sidebar.button("âœ¨ æ¸…ç©ºèŠå¤©è®°å½•"):
    st.session_state.history = []
    st.rerun()

# ä»…å½“RAGå¯ç”¨æ—¶æ˜¾ç¤ºAPIé…ç½®
if st.session_state.rag_enabled:
    st.sidebar.header("ğŸ”¬ æœç´¢è°ƒä¼˜")
    st.session_state.similarity_threshold = st.sidebar.slider(
        "ç›¸ä¼¼åº¦é˜ˆå€¼",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        help="å€¼è¶Šä½è¿”å›çš„æ–‡æ¡£è¶Šå¤šä½†ç›¸å…³æ€§å¯èƒ½è¶Šä½ï¼›å€¼è¶Šé«˜ç›¸å…³æ€§è¦æ±‚è¶Šä¸¥æ ¼ã€‚"
    )

# åœ¨ä¾§è¾¹æ é…ç½®åŒºæ·»åŠ ç½‘é¡µæœç´¢è®¾ç½®ï¼ˆç°æœ‰APIè¾“å…¥ä¹‹åï¼‰
st.sidebar.header("ğŸŒ ç½‘é¡µæœç´¢")
st.session_state.use_web_search = st.sidebar.checkbox("å¯ç”¨ç½‘é¡µæœç´¢ fallback", value=st.session_state.use_web_search)

if st.session_state.use_web_search:
    exa_api_key = st.sidebar.text_input(
        "Exa AI APIå¯†é’¥",
        type="password",
        value=st.session_state.exa_api_key,
        help="å½“æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ—¶ï¼Œéœ€è¦æ­¤å¯†é’¥å¯ç”¨ç½‘é¡µæœç´¢ fallback åŠŸèƒ½"
    )
    st.session_state.exa_api_key = exa_api_key

    # å¯é€‰çš„åŸŸåè¿‡æ»¤
    default_domains = ["arxiv.org", "wikipedia.org", "github.com", "medium.com"]
    custom_domains = st.sidebar.text_input(
        "è‡ªå®šä¹‰åŸŸåï¼ˆé€—å·åˆ†éš”ï¼‰",
        value=",".join(default_domains),
        help="è¾“å…¥è¦æœç´¢çš„åŸŸåï¼Œä¾‹å¦‚ï¼šarxiv.org,wikipedia.org"
    )
    search_domains = [d.strip() for d in custom_domains.split(",") if d.strip()]


# å·¥å…·å‡½æ•°
def init_qdrant() -> QdrantClient | None:
    """åˆå§‹åŒ–æœ¬åœ°Dockeréƒ¨ç½²çš„Qdrantå®¢æˆ·ç«¯ã€‚

    è¿”å›:
        QdrantClient: åˆå§‹åŒ–æˆåŠŸçš„Qdrantå®¢æˆ·ç«¯ã€‚
        None: åˆå§‹åŒ–å¤±è´¥æ—¶è¿”å›Noneã€‚
    """
    try:
        return QdrantClient(url="http://localhost:6333")
    except Exception as e:
        st.error(f"ğŸ”´ Qdrantè¿æ¥å¤±è´¥: {str(e)}")
        return None


# æ–‡æ¡£å¤„ç†å‡½æ•°
def process_pdf(file) -> List:
    """å¤„ç†PDFæ–‡ä»¶å¹¶æ·»åŠ æ¥æºå…ƒæ•°æ®ã€‚"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(file.getvalue())
            loader = PyPDFLoader(tmp_file.name)
            documents = loader.load()

            # æ·»åŠ æ¥æºå…ƒæ•°æ®
            for doc in documents:
                doc.metadata.update({
                    "source_type": "pdf",
                    "file_name": file.name,
                    "timestamp": datetime.now().isoformat()
                })

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            return text_splitter.split_documents(documents)
    except Exception as e:
        st.error(f"ğŸ“„ PDFå¤„ç†é”™è¯¯: {str(e)}")
        return []


def process_web(url: str) -> List:
    """å¤„ç†ç½‘é¡µURLå¹¶æ·»åŠ æ¥æºå…ƒæ•°æ®ã€‚"""
    try:
        loader = WebBaseLoader(
            web_paths=(url,),
            bs_kwargs=dict(
                parse_only=bs4.SoupStrainer(
                    class_=("post-content", "post-title", "post-header", "content", "main")
                )
            )
        )
        documents = loader.load()

        # æ·»åŠ æ¥æºå…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update({
                "source_type": "url",
                "url": url,
                "timestamp": datetime.now().isoformat()
            })

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        return text_splitter.split_documents(documents)
    except Exception as e:
        st.error(f"ğŸŒ ç½‘é¡µå¤„ç†é”™è¯¯: {str(e)}")
        return []


# å‘é‡å­˜å‚¨ç®¡ç†
def create_vector_store(client, texts):
    """åˆ›å»ºå¹¶åˆå§‹åŒ–åŒ…å«æ–‡æ¡£çš„å‘é‡å­˜å‚¨ã€‚"""
    try:
        # æŒ‰éœ€åˆ›å»ºé›†åˆ
        try:
            client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(
                    size=1024,  # ä¸åµŒå…¥æ¨¡å‹ç»´åº¦åŒ¹é…
                    distance=Distance.COSINE
                )
            )
            st.success(f"ğŸ“š åˆ›å»ºæ–°é›†åˆ: {COLLECTION_NAME}")
        except Exception as e:
            if "already exists" not in str(e).lower():
                raise e

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store = QdrantVectorStore(
            client=client,
            collection_name=COLLECTION_NAME,
            embedding=OllamaEmbedderr()
        )

        # æ·»åŠ æ–‡æ¡£
        with st.spinner('ğŸ“¤ æ­£åœ¨å°†æ–‡æ¡£ä¸Šä¼ åˆ°Qdrant...'):
            vector_store.add_documents(texts)
            st.success("âœ… æ–‡æ¡£å­˜å‚¨æˆåŠŸï¼")
            return vector_store

    except Exception as e:
        st.error(f"ğŸ”´ å‘é‡å­˜å‚¨é”™è¯¯: {str(e)}")
        return None


def get_web_search_agent() -> Agent:
    """åˆå§‹åŒ–ç½‘é¡µæœç´¢ä»£ç†ã€‚"""
    return Agent(
        name="ç½‘é¡µæœç´¢ä»£ç†",
        model=Ollama(id="llama3.2"),
        tools=[ExaTools(
            api_key=st.session_state.exa_api_key,
            include_domains=search_domains,
            num_results=5
        )],
        instructions="""ä½ æ˜¯ç½‘é¡µæœç´¢ä¸“å®¶ï¼Œä»»åŠ¡å¦‚ä¸‹ï¼š
        1. æ ¹æ®æŸ¥è¯¢å…³é”®è¯æœç´¢ç›¸å…³ç½‘é¡µä¿¡æ¯
        2. æ•´ç†å¹¶æ€»ç»“æœ€ç›¸å…³çš„ä¿¡æ¯å†…å®¹
        3. åœ¨å›å¤ä¸­åŒ…å«ä¿¡æ¯æ¥æºé“¾æ¥
        """,
        show_tool_calls=True,
        markdown=True,
    )


def get_rag_agent() -> Agent:
    """åˆå§‹åŒ–ä¸»RAGä»£ç†ã€‚"""
    return Agent(
        name="Qwen 3 RAGä»£ç†",
        model=Ollama(id=st.session_state.model_version),
        instructions="""ä½ æ˜¯ä¸“æ³¨äºæä¾›å‡†ç¡®ç­”æ¡ˆçš„æ™ºèƒ½ä»£ç†ã€‚

        å½“æ”¶åˆ°é—®é¢˜æ—¶ï¼š
        - åˆ†æé—®é¢˜å¹¶åˆ©ç”¨å·²æœ‰çŸ¥è¯†å›ç­”
        - è‹¥æä¾›æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆåŸºäºæ–‡æ¡£å†…å®¹å›ç­”
        - å›ç­”éœ€ç²¾ç¡®å¹¶å¼•ç”¨å…·ä½“ç»†èŠ‚

        å½“æ”¶åˆ°ç½‘é¡µæœç´¢ç»“æœæ—¶ï¼š
        - æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºäºç½‘é¡µæœç´¢
        - æ¸…æ™°æ•´åˆæœç´¢åˆ°çš„ä¿¡æ¯

        å§‹ç»ˆä¿æŒå›ç­”çš„é«˜å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ã€‚
        """,
        show_tool_calls=True,
        markdown=True,
    )


def check_document_relevance(query: str, vector_store, threshold: float = 0.7) -> tuple[bool, List]:
    """æ£€æŸ¥æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆåŸä»£ç ä¸­å®šä¹‰ä½†æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºåç»­æ‰©å±•ï¼‰ã€‚

    å‚æ•°:
        query (str): ç”¨æˆ·æŸ¥è¯¢
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
        threshold (float): ç›¸å…³æ€§é˜ˆå€¼

    è¿”å›:
        tuple[bool, List]: ç›¸å…³æ€§ç»“æœï¼ˆæ˜¯å¦ç›¸å…³ï¼Œç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼‰
    """
    if not vector_store:
        return False, []

    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 5, "score_threshold": threshold}
    )
    docs = retriever.invoke(query)
    return bool(docs), docs


# å¸ƒå±€ï¼šèŠå¤©è¾“å…¥æ¡†ä¸ç½‘é¡µæœç´¢å¼ºåˆ¶åˆ‡æ¢æŒ‰é’®
chat_col, toggle_col = st.columns([0.9, 0.1])

with chat_col:
    prompt = st.chat_input("è¯·æé—®ï¼ˆå·²å¯ç”¨RAGï¼Œå¯è¯¢é—®æ–‡æ¡£ç›¸å…³é—®é¢˜ï¼‰" if st.session_state.rag_enabled else "è¯·æé—®...")

with toggle_col:
    st.session_state.force_web_search = st.toggle('ğŸŒ', help="å¼ºåˆ¶ä½¿ç”¨ç½‘é¡µæœç´¢")

# æ£€æŸ¥RAGæ˜¯å¦å¯ç”¨
if st.session_state.rag_enabled:
    qdrant_client = init_qdrant()

    # --- æ–‡æ¡£ä¸Šä¼ åŒºåŸŸï¼ˆç§»è‡³ä¸»å†…å®¹åŒºï¼‰---
    with st.expander("ğŸ“ ä¸Šä¼ æ–‡æ¡£æˆ–URLç”¨äºRAG", expanded=False):
        if not qdrant_client:
            st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ é…ç½®Qdrant APIå¯†é’¥å’ŒURLï¼Œä»¥å¯ç”¨æ–‡æ¡£å¤„ç†åŠŸèƒ½ã€‚")
        else:
            uploaded_files = st.file_uploader(
                "ä¸Šä¼ PDFæ–‡ä»¶",
                accept_multiple_files=True,
                type='pdf'
            )
            url_input = st.text_input("è¾“å…¥ç½‘é¡µURLä»¥æŠ“å–å†…å®¹")

            if uploaded_files:
                st.write(f"æ­£åœ¨å¤„ç† {len(uploaded_files)} ä¸ªPDFæ–‡ä»¶...")
                all_texts = []
                for file in uploaded_files:
                    if file.name not in st.session_state.processed_documents:
                        with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}... "):
                            texts = process_pdf(file)
                            if texts:
                                all_texts.extend(texts)
                                st.session_state.processed_documents.append(file.name)
                    else:
                        st.write(f"ğŸ“„ {file.name} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ã€‚")

                if all_texts:
                    with st.spinner("æ­£åœ¨åˆ›å»ºå‘é‡å­˜å‚¨..."):
                        st.session_state.vector_store = create_vector_store(qdrant_client, all_texts)

            if url_input:
                if url_input not in st.session_state.processed_documents:
                    with st.spinner(f"æ­£åœ¨æŠ“å–å¹¶å¤„ç† {url_input}..."):
                        texts = process_web(url_input)
                        if texts:
                            st.session_state.vector_store = create_vector_store(qdrant_client, texts)
                            st.session_state.processed_documents.append(url_input)
                else:
                    st.write(f"ğŸ”— {url_input} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ã€‚")

            if st.session_state.vector_store:
                st.success("å‘é‡å­˜å‚¨å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æé—®ã€‚")
            elif not uploaded_files and not url_input:
                st.info("è¯·ä¸Šä¼ PDFæ–‡ä»¶æˆ–è¾“å…¥URLï¼Œä»¥æ„å»ºå‘é‡å­˜å‚¨ã€‚")

    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºå·²å¤„ç†çš„æ¥æº
    if st.session_state.processed_documents:
        st.sidebar.header("ğŸ“š å·²å¤„ç†æ¥æº")
        for source in st.session_state.processed_documents:
            if source.endswith('.pdf'):
                st.sidebar.text(f"ğŸ“„ {source}")
            else:
                st.sidebar.text(f"ğŸŒ {source}")

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt:
    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
    st.session_state.history.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    if st.session_state.rag_enabled:
        # ç°æœ‰RAGæµç¨‹ä¿æŒä¸å˜
        with st.spinner("ğŸ¤” æ­£åœ¨åˆ†ææŸ¥è¯¢..."):
            try:
                rewritten_query = prompt  # æ­¤å¤„å¯æ‰©å±•æŸ¥è¯¢é‡å†™é€»è¾‘
                with st.expander("æŸ¥è¯¢åˆ†æè¯¦æƒ…"):
                    st.write(f"ç”¨æˆ·åŸå§‹æŸ¥è¯¢: {prompt}")
            except Exception as e:
                st.error(f"âŒ æŸ¥è¯¢é‡å†™é”™è¯¯: {str(e)}")
                rewritten_query = prompt

        # æ­¥éª¤2ï¼šæ ¹æ®å¼ºåˆ¶ç½‘é¡µæœç´¢åˆ‡æ¢æŒ‰é’®é€‰æ‹©æœç´¢ç­–ç•¥
        context = ""
        docs = []
        if not st.session_state.force_web_search and st.session_state.vector_store:
            # ä¼˜å…ˆå°è¯•æ–‡æ¡£æœç´¢
            retriever = st.session_state.vector_store.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "k": 5,
                    "score_threshold": st.session_state.similarity_threshold
                }
            )
            docs = retriever.invoke(rewritten_query)
            if docs:
                context = "\n\n".join([d.page_content for d in docs])
                st.info(f"ğŸ“Š æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆç›¸ä¼¼åº¦ > {st.session_state.similarity_threshold}ï¼‰")
            elif st.session_state.use_web_search:
                st.info("ğŸ”„ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ­£åœ¨åˆ‡æ¢åˆ°ç½‘é¡µæœç´¢...")

        # æ­¥éª¤3ï¼šæ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶ä½¿ç”¨ç½‘é¡µæœç´¢ï¼š
        # 1. å¼ºåˆ¶ç½‘é¡µæœç´¢æŒ‰é’®å·²å¼€å¯ï¼Œæˆ–
        # 2. æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¸”è®¾ç½®ä¸­å·²å¯ç”¨ç½‘é¡µæœç´¢
        if (
                st.session_state.force_web_search or not context) and st.session_state.use_web_search and st.session_state.exa_api_key:
            with st.spinner("ğŸ” æ­£åœ¨ç½‘é¡µæœç´¢..."):
                try:
                    web_search_agent = get_web_search_agent()
                    web_results = web_search_agent.run(rewritten_query).content
                    if web_results:
                        context = f"ç½‘é¡µæœç´¢ç»“æœ:\n{web_results}"
                        if st.session_state.force_web_search:
                            st.info("â„¹ï¸ å·²æŒ‰è¯·æ±‚ä½¿ç”¨ç½‘é¡µæœç´¢ã€‚")
                        else:
                            st.info("â„¹ï¸ å› æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå·²ä½¿ç”¨ç½‘é¡µæœç´¢ fallbackã€‚")
                except Exception as e:
                    st.error(f"âŒ ç½‘é¡µæœç´¢é”™è¯¯: {str(e)}")

        # æ­¥éª¤4ï¼šä½¿ç”¨RAGä»£ç†ç”Ÿæˆå›ç­”
        with st.spinner("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            try:
                rag_agent = get_rag_agent()

                if context:
                    full_prompt = f"""ä¸Šä¸‹æ–‡: {context}

åŸå§‹é—®é¢˜: {prompt}
è¯·åŸºäºå¯ç”¨ä¿¡æ¯æä¾›å…¨é¢çš„å›ç­”ã€‚"""
                else:
                    full_prompt = f"åŸå§‹é—®é¢˜: {prompt}\n"
                    st.info("â„¹ï¸ åœ¨æ–‡æ¡£å’Œç½‘é¡µæœç´¢ä¸­å‡æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

                response = rag_agent.run(full_prompt)

                # å°†åŠ©æ‰‹å›ç­”æ·»åŠ åˆ°å†å²è®°å½•
                st.session_state.history.append({
                    "role": "assistant",
                    "content": response.content
                })

                # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
                with st.chat_message("assistant"):
                    st.write(response.content)

                    # è‹¥å­˜åœ¨ç›¸å…³æ–‡æ¡£ï¼Œæ˜¾ç¤ºæ¥æºï¼ˆæœªå¼ºåˆ¶ç½‘é¡µæœç´¢æ—¶ï¼‰
                    if not st.session_state.force_web_search and 'docs' in locals() and docs:
                        with st.expander("ğŸ” æŸ¥çœ‹æ–‡æ¡£æ¥æº"):
                            for i, doc in enumerate(docs, 1):
                                source_type = doc.metadata.get("source_type", "unknown")
                                source_icon = "ğŸ“„" if source_type == "pdf" else "ğŸŒ"
                                source_name = doc.metadata.get("file_name" if source_type == "pdf" else "url",
                                                               "unknown")
                                st.write(f"{source_icon} æ¥æº {i}ï¼ˆ{source_name}ï¼‰:")
                                st.write(f"{doc.page_content[:200]}...")

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå›ç­”é”™è¯¯: {str(e)}")

    else:
        # æ— RAGçš„ç®€å•æ¨¡å¼
        with st.spinner("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            try:
                rag_agent = get_rag_agent()
                web_search_agent = get_web_search_agent() if st.session_state.use_web_search else None

                # è‹¥å¼ºåˆ¶æˆ–å¯ç”¨ç½‘é¡µæœç´¢ï¼Œå…ˆæ‰§è¡Œç½‘é¡µæœç´¢
                context = ""
                if st.session_state.force_web_search and web_search_agent:
                    with st.spinner("ğŸ” æ­£åœ¨ç½‘é¡µæœç´¢..."):
                        try:
                            web_results = web_search_agent.run(prompt).content
                            if web_results:
                                context = f"ç½‘é¡µæœç´¢ç»“æœ:\n{web_results}"
                                st.info("â„¹ï¸ å·²æŒ‰è¯·æ±‚ä½¿ç”¨ç½‘é¡µæœç´¢ã€‚")
                        except Exception as e:
                            st.error(f"âŒ ç½‘é¡µæœç´¢é”™è¯¯: {str(e)}")

                # ç”Ÿæˆå›ç­”
                if context:
                    full_prompt = f"""ä¸Šä¸‹æ–‡: {context}

é—®é¢˜: {prompt}

è¯·åŸºäºå¯ç”¨ä¿¡æ¯æä¾›å…¨é¢çš„å›ç­”ã€‚"""
                else:
                    full_prompt = prompt

                response = rag_agent.run(full_prompt)
                response_content = response.content

                # æå–æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
                import re

                think_pattern = r'(.*?)'
                think_match = re.search(think_pattern, response_content, re.DOTALL)

                if think_match:
                    thinking_process = think_match.group(1).strip()
                    final_response = re.sub(think_pattern, '', response_content, flags=re.DOTALL).strip()
                else:
                    thinking_process = None
                    final_response = response_content

                # å°†åŠ©æ‰‹çš„æœ€ç»ˆå›ç­”æ·»åŠ åˆ°å†å²è®°å½•
                st.session_state.history.append({
                    "role": "assistant",
                    "content": final_response
                })

                # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
                with st.chat_message("assistant"):
                    if thinking_process:
                        with st.expander("ğŸ¤” æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹"):
                            st.markdown(thinking_process)
                    st.markdown(final_response)

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå›ç­”é”™è¯¯: {str(e)}")

else:
    st.warning("æ‚¨å¯ä»¥ç›´æ¥ä¸æœ¬åœ°çš„qwenå’Œgemmaæ¨¡å‹å¯¹è¯ï¼åˆ‡æ¢RAGæ¨¡å¼å¯ä¸Šä¼ æ–‡æ¡£è¿›è¡Œé—®ç­”ï¼")